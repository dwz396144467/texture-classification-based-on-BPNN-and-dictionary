1。特征提取
首先对文本信息进行分词处理，采用基于字符串匹配的方法：
假如一段叫：李二狗就是一个傻逼
基于匹配的方法就是依次截取一到多个词，并与字典库进行匹配。如二狗，如果匹配到字典中有这个词，则将其分为一个词；当取到“狗就”，发现字典中没有与之匹配的，则说明这个不是一个词语，进行顺序操作，最优将这段话分为：李  二狗  就是  一个  傻逼。
2. 得到分词后的文本之后，就是转换成数字编码，因此电脑没办法识别汉字。这一部分叫特征表示，即用数字的方式表示中文文本，采用的方法是基于词带模型的特征表示：
词带就是字典--程序中那个dictionary.mat。我们将分词处理之后的文本中的每一个词语，分别与字典中的词进行匹配，只要出现过就为1，否则为0。
如 字典中的词含有：李 周 吴 郑 王 他妈的 就是  大 傻逼 一个  三炮 也是 瓜娃子，一共13词（当然正常的词典都是上万个词），将1中得到的词语与之匹配，则李二狗就是一个傻逼对应的数字编码就应该是 1 0 0 0 0 0 1 0 1 1 0 0 0 


3，通过2我们将文本表示成了数字，但是这样的表示通常都是稀疏的（因为一般字典都含有上万个词，所以得到的数字表示大部分都是0），为此我们利用降维方法，消除掉这些冗余特征。这里我们采用的PCA（主成分分析）进行降维，并降至15维。
4. 文本分类，采用的就是bp网络


代码修改的地方不多，主要就是超参数的选择，（1）如pca的降维数，维数过高，包含冗余数据，过低又会删除掉重要信息。（2）bp网络结构的调整，如隐含层节点数，学习率，等


